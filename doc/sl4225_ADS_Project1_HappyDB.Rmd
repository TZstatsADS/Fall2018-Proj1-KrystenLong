---
title: "Happy Moments"
author: "Shiqing Long"
output:
  html_document: default
  html_notebook: default
  word_document: default
---

HappyDB is a corpus of 100,000 crowd-sourced happy moments via Amazon's Mechanical Turk. You can read more about it on https://arxiv.org/abs/1801.07746.

Here, we explore this data set and try to answer the question, "What makes people happy?"

# What makes people happy?
In this report, we explored what makes people happy using NLP and Text Mining. First, we studied the demographic information to view distribution of response. Second, we segment population according to status to compare deffierent categories. Then we appplied wordcloud (both single word and bigrams) to figure out the what makes us happy, also, we compared time period to see if there is any difference in happy moment. Last, we use topic modeling to summarize the things and senario that make people happy and visualize the topic distribution.

# Step 0 - Load all the required libraries

From the packages' descriptions:

+ `tidyverse` is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures;
+ `tidytext` allows text mining using 'dplyr', 'ggplot2', and other tidy tools;
+ `DT` provides an R interface to the JavaScript library DataTables;
+ `scales` map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends;
+ `wordcloud2` provides an HTML5 interface to wordcloud for data visualization;
+ `gridExtra` contains miscellaneous functions for "grid" graphics;
+ `ngram` is for constructing n-grams (“tokenizing”), as well as generating new text based on the n-gram structure of a given text input (“babbling”);
+ `Shiny` is an R package that makes it easy to build interactive web apps straight from R;

```{r load libraries, warning=FALSE, message=FALSE}
devtools::install_github("lchiffon/wordcloud2")
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(gridExtra)
library(ngram)
library(ggplot2)
library(wordcloud2)
library(tidyr)
library(reshape2)
library(tm)
library(topicmodels)
```

## Step 1 - Load the processed text data along with demographic information on contributors
We use the processed data for our analysis and combine it with the demographic information available.
```{r load data, warning=FALSE, message=FALSE}
hm_data <- read_csv("../output/processed_moments.csv")

urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
```

### Combine both the data sets and keep the required columns for analysis
We select a subset of the data that satisfies specific row conditions.
```{r combining data, warning=FALSE, message=FALSE}
hm_data <- hm_data %>%
  inner_join(demo_data, by = "wid") %>%
  select(wid,
         original_hm,
         cleaned_hm,
         gender, 
         marital, 
         parenthood,
         reflection_period,
         age, 
         country, 
         ground_truth_category, 
         text) %>%
  mutate(count = sapply(hm_data$text, wordcount)) %>%
  filter(gender %in% c("m", "f")) %>%
  filter(marital %in% c("single", "married")) %>%
  filter(parenthood %in% c("n", "y")) %>%  filter(reflection_period %in% c("24h", "3m")) %>%
  mutate(reflection_period = fct_recode(reflection_period,    #Change factor levels by hand
                                        months_3 = "3m", hours_24 = "24h"))

```


## Step 2 - Explorary Data Analysis on demographic distribution
###Area vs Gender disrtibution
Although the dataset contains population from more than 30 countries, the majority of people are from the US and India, others can be ignored. Therefore only visualize these population.
```{r}
df <- hm_data%>%select(gender, marital,parenthood, country,age) %>% 
      filter(country %in% c("USA", "IND")) %>% 
      group_by(country, gender) %>%  summarise(n = n())

      

ggplot(df,aes(x = country, y = n,fill = gender))+
   geom_bar(stat="identity",position = "dodge")
```

The people in USA and India consist of most of population, nearly 95.27%. However, population composed with imbalance gender, males are larger than females.

###Marital vs Parenthood
Although a minor part of population are separeted, divorced, widowed, for the same reson, we only take the majority of populayion into account, that is single and married.
```{r}
df <- hm_data%>%select(gender, marital,parenthood) %>% 
      filter(parenthood %in% c("y", "n")) %>%
      filter(marital %in% c("single", "married")) %>%
      group_by(marital, parenthood) %>%  summarise(n = n())

ggplot(df,aes(x = marital, y = n,fill = parenthood))+
   geom_bar(stat="identity",position = "dodge")
```

Looking at the status of population, the single are little more than the married. Especially, single with no children and married with children contribute the majority among total population, nearly 83.79%.

## Step 3 - Different emotional categories among different status
Segment population into 8 profiles by demogrphic features,using both gender,martal and parenthood features, analyze the different emotional categories between 8 profiles.
```{r}
df_family <- hm_data%>%select(gender, marital,parenthood,ground_truth_category) %>% 
      filter(is.na(ground_truth_category) == FALSE) %>%
      unite(status, gender, marital,parenthood, sep  = "_" ) %>%
      group_by(status, ground_truth_category) %>% 
      summarise(n = n())%>% mutate(proportion = n / sum(n))

ggplot(df_family,aes(x = status, y = proportion,fill = ground_truth_category))+
   geom_bar(stat="identity",position = "dodge")
```

Segment population into 8 profiles using characters like gender, marital, and parenthood. As is shown in the barchart, affection and achievement are two dominate category in happy feeling, most people remembered their happy time when they obtained affection and achievement. In addition, we can compare the difference between different profiles. Although for the majority of profiles, affection was a dominate category in their happy moment, however, for single male, no matter have children or not, their happy moment most came from achievement. Compared to other profile, the married with no children, their happly moment were largely derived from leisure.

## Step 4 - Explorary Data Analysis on Word Cloud
###4.1Total text exploration using single word
Split total text into every single word, creating a bag of words using the text data
```{r bag of words, warning=FALSE, message=FALSE}
bag_of_words <-  hm_data %>%
  unnest_tokens(word, text)   #Split a column into tokens using the tokenizers package


word_count <- bag_of_words %>%  #high-frenquency word list 
  count(word, sort = TRUE)

#select top 200
word_count <- word_count[1:200,]
```

Delete meaningless words to make the wordcloud more insightful
```{r}
set.seed(1234)
#delete meaningless words like "day, time"
word_count <- word_count[c(-2,-3),]
wordcloud2(word_count,size =0.5)
```

Using single word, happy moment are largely derive from friendship. In addition,  family(wife,husband, son, daughter, mom, brother,sister), brithday, job, games also provide happy memories. 

Plot barchart to visulize top 10 words.
```{r}
ggplot(head(word_count,10), aes(reorder(word,n), n, fill = 1:10))+
  geom_bar(stat = "identity")+coord_flip() +
  xlab("Single Word")+ylab("Frequency")+
  ggtitle("Most Frequent Word")
```

What made you happy today? Uing single word, most of people think the friendship made them happy, then family, home. 

However, somethings that aroused happy moment can not be expressed by a single words, so using single word miss some important information. Then we try bigrams.


###4.2 Total text exploration using bigrams
```{r bigram, warning=FALSE, message=FALSE}
hm_bigrams <- hm_data %>%
  filter(count != 1) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigram_counts <- hm_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% #two-words list
  count(word1, word2, sort = TRUE)

#library(tidyr)
word_count_bigrams <- unite(bigram_counts, word, word1, word2, sep =" ")
word_count_bigrams <- word_count_bigrams[1:100,]
```


```{r}
wordcloud2(word_count_bigrams,size =0.3)
```

What made you happy today? Uing bigrams, the moment of the spend time, vedio game, eating ice cream, watching movie, mother day and birthday party made them happy most. Moreover, favorite resturant, buying car, reading a book, spending time with people, grocery store provide happy memory for people

Plot barchart to visulize top 10 bigram words.
```{r}
ggplot(head(word_count_bigrams,10), aes(reorder(word,n), n, fill = 1:10)) +
  geom_bar(stat = "identity") + coord_flip() +
  xlab("Bigrams") + ylab("Frequency") +
  ggtitle("Most Frequent bigrams")
```


###4.3 3 months or 24 hours on wordcloud
We display the difference on word cloud taking account to the reflection period using sigle word and biagrams.
```{r}
bag_of_words_3 <-  hm_data %>%
  filter(reflection_period %in% c("months_3") )%>%
  unnest_tokens(word, text)   #Split a column into tokens using the tokenizers package


word_count_3 <- bag_of_words_3 %>%  #high-frenquency word list 
  count(word, sort = TRUE)

#select top 200
word_count_3 <- word_count_3[1:200,]


bag_of_words_24 <-  hm_data %>%
  filter(reflection_period %in% c("hours_24") )%>%
  unnest_tokens(word, text)   #Split a column into tokens using the tokenizers package


word_count_24 <- bag_of_words_24 %>%  #high-frenquency word list 
  count(word, sort = TRUE)

#select top 200
word_count_24 <- word_count_24[1:200,]

#plot wordcloud

wordcloud2(word_count_3,size =0.5)
```

```{r}
wordcloud2(word_count_24,size =0.5)
```

Compared two wordcloud, family, job, birthday are mentioned words in the happy moment in the past 3 month, words like morning, dinner are more popular in the past 24 hours.

```{r}
hm_bigrams_3 <- hm_data %>%
  filter(count != 1) %>%
  filter(reflection_period %in% c("months_3") )%>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigram_counts_3 <- hm_bigrams_3 %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% #two-words list
  count(word1, word2, sort = TRUE)

#library(tidyr)
word_count_bigrams_3 <- unite(bigram_counts_3, word, word1, word2, sep =" ")
word_count_bigrams_3 <- word_count_bigrams_3[1:100,]

wordcloud2(word_count_bigrams_3,size =0.3)
```

Using bigram to analyze happy moment in the past 3 month, mother day are most frequent bigram. In this three month period, saving money, loosing pounds, visting friends, couple week and college graduation also create happy moment.

```{r}
hm_bigrams_24 <- hm_data %>%
  filter(count != 1) %>%
  filter(reflection_period %in% c("hours_24") )%>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigram_counts_24 <- hm_bigrams_24 %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% #two-words list
  count(word1, word2, sort = TRUE)

#library(tidyr)
word_count_bigrams_24 <- unite(bigram_counts_24, word, word1, word2, sep =" ")
word_count_bigrams_24 <- word_count_bigrams_24[1:100,]
wordcloud2(word_count_bigrams_24,size =0.3)
```

Using bigram to analyze happy moment in the past 24 hours, vedio game, watched movie, talked with friend are mentioned by most of people. However, these little things fade away quickly.We can also notice from the wordcloud that little things like reading a book, walking dog, phone call, night sleep also made people feel happy. 

## Step 5 - Topic Modeling - identifying what topic the words come from
###Build corpus
```{r}
set.seed(1)
hm_data <- hm_data[sample(1:nrow(hm_data), 20000),]
# compute document term matrix with terms 
corpus <- VCorpus(VectorSource(hm_data$text))

```

###Model calculation
```{r}
DTM <- DocumentTermMatrix(corpus)
dim(DTM)

#due to vocabulary pruning, we have empty rows in our DTM, LDA does not like this. So remove them
rowTotals <- apply(DTM , 1, sum)
DTM  <- DTM[rowTotals> 0, ]
```

The topic model inference results in two (approximate) posterior probability distributions: a distribution theta over K topics within each document and a distribution beta over V terms within each topic, where V represents the length of the vocabulary of the collection (V = 9017). Let’s take a closer look at these results:
```{r}
#Number of topics
k <- 7
                                            
#Run LDA using Gibbs sampling
topicModel <- LDA(DTM, k, method="Gibbs", control=list(iter = 500, seed = 1234))
terms(topicModel, 10)

#have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
# topics are probability distribtions over the entire vocabulary
beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms
nDocs(DTM)   
theta <- tmResult$topics 
dim(theta)    

topicNames <- c("affection", "enjoy the moment", "exercise", "leisure", "achievement", "nature", "boding")
```

Using hot words to nanme these 7 topics as "affection", "enjoy the moment", "exercise", "leisure", "achievement", "nature", "boding".

###Visualization of Topics
Then, we select 3 examples to visualize topics.
```{r}
exampleIds <- c(1, 800, 1200)
lapply(corpus[exampleIds], as.character)
```

```{r}
N <- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")  

ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)
```
Form the barchart, we can see the topic distributions within the text. Longer bar implies higher possibility in that topic. 


# Summary

### 1.Most people feel happy when obtain affection and achievement. 
Segment population into 8 profiles using characters like gender, marital, and parenthood. Most people remembered their happy time when they obtained affection and achievement.Their affection are most from interaction between people, their achievement are most from job, house and things with husband & girlfriend.

### 2. Friendship and family members are the most common energy of happy.
Friendship are most common words referred by the whole population. People liked the time to spend togerther, play vidio games and watch movies. Family words like wife, husband, son, mom and daughter are also what made people happy.

### 3.Happy memory for single male most are from achievement,the married are from leisure.
Although for the majority of profiles, affection was a dominate category in their happy moment, single male, no matter have children or not, their happy moment most came from achievement. Compared to other profile, the married with no children, their happly moment were largely derived from leisure.

### 4.Even a small things provided subtle happiness, unforgetable events made people happy in longer time period.
Lifes are full of happiness, small or big, very subtle or unforgetable. In the 24 hour period, people felt happy about even little things like having a good moring, dinner or walking dog. But in the longer period, like 3 months, they were happy about achievement like getting a job, graduation, saving money and loosing pounds.

